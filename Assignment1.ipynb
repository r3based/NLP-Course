{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoyNtLwmkUVW"
      },
      "source": [
        "# Natural Language Processing. Assignment 1. Tokenization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kygkn1jMkaT9"
      },
      "source": [
        "In this assignment, you need to implement, train, and analyze a Byte-Pair Encoding (BPE) tokenizer.\n",
        "\n",
        "The assignment consist of 3 tasks. When you finish all the tasks, create a GitHub repository for this assignment (you can use this repository later for the other assignments) and submit this notebook in the repository. Leave `requirements.txt` file if your code requires additional installations. Submit the link to the repository in Moodle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJokC1PFqhNe"
      },
      "source": [
        "## Task 1: Data Preparation and Vocabulary Size Selection (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUbPSf6pqrcF"
      },
      "source": [
        "First, load the [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus). After loading the corpus, you need to select the appropriate vocabulary size for the BPE tokenizer. The appropriate vocabulary size is the minimal vocabulary size that covers at least 90% of the words in the corpus. The coverage is calculated according to the following formula:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbzsvC77u0We"
      },
      "source": [
        "$$ \\text{coverage}(k) = \\frac{\\sum_{r=1}^{k} f(r)}{\\sum_{r=1}^{N} f(r)} $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIjntOQDu1ed"
      },
      "source": [
        "where $f(r)$ is the frequency of the top-$r$ word, $k$ is the number of top-$k$ tokens included in vocab, $N$ is the total unique words in corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtunZHVEvNuF"
      },
      "source": [
        "So, for this task you need to do the following:\n",
        "\n",
        "1. Load the Brown corpus (0.5 points)\n",
        "2. Plot cumulative coverage vs. vocabulary size for the loaded corpus (1 point)\n",
        "3. Select the appropriate vocabulary size (0.5 point)\n",
        "4. Answer the questions:\n",
        "    1. Why the coverage slows down the increase as the vocabulary size increases? (0.5 point)\n",
        "    2. Which empirical law explains the slowing down increase of the coverage? (0.5 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94OTtlKZl13I"
      },
      "source": [
        "## Task 2: Implement Byte-Pair Encoding (BPE) Tokenizer (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uphIHqoEmDLH"
      },
      "source": [
        "Implement the [BPE tokenizer](https://arxiv.org/pdf/1508.07909) as the `BPETokenizer` class.\n",
        "\n",
        "The class should contain correctly implemented:\n",
        "\n",
        "* `train` method (1.5 points).\n",
        "* `tokenize` method (1.5 points).\n",
        "\n",
        "The code should have docstrings and comments (1 point)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhhWPld0zwjG"
      },
      "source": [
        "## Task 3: Tokenizer Training and Analysis (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLkP3Sofz_xV"
      },
      "source": [
        "1. Train the `BPETokenizer` on the Brown corpus with the appropriate vocabulary size selected in Task 1 (1 points)\n",
        "2. Use the Brown corpus (1000 samples) to calculate the mean and standard deviation of\n",
        "    * tokenizer's fertility (1 points)\n",
        "    * length of the tokenized sentence (1 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grading Procedure Details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "During the grading of the completed assignments, a random set of students will be sampled for the **offline assignment defence**. The defence will be arranged shortly after the assignment submission deadline. The particular date and time will be announced later. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The aim of the assignment defence is to ensure the students understand well their own solutions and know how thier solution works. To check this, the students will be asked various questions about the provided solution. In addition, the students will be asked to run their solution to ensure the solution works without errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Examples of questions:\n",
        "\n",
        "1. How the cumulative coverage is calculated? Why is it called cumulative?\n",
        "2. What is the rank of a word?\n",
        "3. How does the BPE tokenizer work? Note: for this question, the students will not be able to see the their own implementation.\n",
        "4. Why do you consider such vocabulary size appropriate?\n",
        "5. What is the formula for the fertility of the tokenizer?\n",
        "6. How do you perform pre-tokenization in your implementation?\n",
        "7. How do you handle stopwords in the training corpus? Why?\n",
        "8. etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a result of the assignment defence, the grade for the assignment may be adjusted."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
